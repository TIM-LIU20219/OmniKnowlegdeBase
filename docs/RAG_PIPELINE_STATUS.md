# RAG Pipeline å®ŒæˆçŠ¶æ€æ€»ç»“

## âœ… å·²å®Œæˆç»„ä»¶

### 1. æ£€ç´¢ç»„ä»¶
- âœ… **EmbeddingService**: æ–‡æœ¬å‘é‡åŒ–ï¼ˆæ”¯æŒå¤šç§æ¨¡å‹ï¼ŒGPU/CPUè‡ªåŠ¨æ£€æµ‹ï¼‰
- âœ… **VectorService**: ChromaDBå‘é‡å­˜å‚¨å’ŒæŸ¥è¯¢
- âœ… **ChunkingService**: æ–‡æ¡£åˆ†å—ï¼ˆå¤šç§ç­–ç•¥ï¼‰
- âœ… **ChromaDBRetriever**: LangChainå…¼å®¹çš„æ£€ç´¢å™¨åŒ…è£…

### 2. ç”Ÿæˆç»„ä»¶
- âœ… **LLMService**: LLMé›†æˆï¼ˆæ”¯æŒDeepSeek/OpenRouter/OpenAIï¼‰
- âœ… **RAGService**: å®Œæ•´çš„RAG pipelineï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰

### 3. è¯„ä¼°ç»„ä»¶
- âœ… **BenchmarkEvaluator**: RAG benchmarkè¯„ä¼°ç³»ç»Ÿ
- âœ… **è¯„ä¼°æŒ‡æ ‡**: Precision/Recall/F1ã€ç­”æ¡ˆç›¸ä¼¼åº¦ç­‰
- âœ… **è¯„ä¼°è„šæœ¬**: è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹

## ğŸ“Š RAG Pipeline å®Œæ•´æµç¨‹

```
ç”¨æˆ·é—®é¢˜
    â†“
[æ£€ç´¢é˜¶æ®µ]
1. EmbeddingService: é—®é¢˜å‘é‡åŒ–
2. ChromaDBRetriever: è¯­ä¹‰æœç´¢ç›¸å…³æ–‡æ¡£
3. ä¸Šä¸‹æ–‡æ ¼å¼åŒ–: åˆå¹¶æ£€ç´¢åˆ°çš„æ–‡æ¡£
    â†“
[ç”Ÿæˆé˜¶æ®µ]
4. Promptæ„å»º: å°†é—®é¢˜å’Œä¸Šä¸‹æ–‡ç»„åˆ
5. LLMService: LLMç”Ÿæˆç­”æ¡ˆ
    â†“
è¿”å›ç­”æ¡ˆ + æ¥æºä¿¡æ¯
```

## ğŸ” è·ç¦»å®Œæ•´RAG Pipelineçš„Gapåˆ†æ

### å·²å…·å¤‡çš„èƒ½åŠ›

1. **æ–‡æ¡£å¤„ç†** âœ…
   - æ–‡æ¡£è§£æï¼ˆPDF/Markdown/URLï¼‰
   - æ–‡æ¡£åˆ†å—
   - å‘é‡åŒ–å­˜å‚¨

2. **æ£€ç´¢èƒ½åŠ›** âœ…
   - è¯­ä¹‰æœç´¢
   - å…ƒæ•°æ®è¿‡æ»¤
   - ç›¸ä¼¼åº¦é˜ˆå€¼

3. **ç”Ÿæˆèƒ½åŠ›** âœ…
   - LLMé›†æˆ
   - Promptæ¨¡æ¿
   - æµå¼å“åº”

4. **è¯„ä¼°èƒ½åŠ›** âœ…
   - Benchmarkæ•°æ®é›†
   - è‡ªåŠ¨åŒ–è¯„ä¼°
   - å¤šç»´åº¦æŒ‡æ ‡

### æ½œåœ¨å¢å¼ºç‚¹ï¼ˆå¯é€‰ï¼‰

1. **é«˜çº§æ£€ç´¢ç­–ç•¥**
   - [ ] æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰
   - [ ] æŸ¥è¯¢æ‰©å±•ï¼ˆquery expansionï¼‰
   - [ ] é‡æ’åºï¼ˆrerankingï¼‰

2. **ä¸Šä¸‹æ–‡ä¼˜åŒ–**
   - [ ] ä¸Šä¸‹æ–‡å‹ç¼©
   - [ ] ç›¸å…³æ€§è¿‡æ»¤
   - [ ] åŠ¨æ€ä¸Šä¸‹æ–‡é•¿åº¦

3. **å¯¹è¯ç®¡ç†**
   - [ ] å¤šè½®å¯¹è¯æ”¯æŒ
   - [ ] å¯¹è¯å†å²ç®¡ç†
   - [ ] ä¸Šä¸‹æ–‡çª—å£æ»‘åŠ¨

4. **é«˜çº§è¯„ä¼°**
   - [ ] BLEU/ROUGEåˆ†æ•°
   - [ ] è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆåŸºäºembeddingï¼‰
   - [ ] äººå·¥è¯„ä¼°æ¥å£

5. **æ€§èƒ½ä¼˜åŒ–**
   - [ ] æ‰¹é‡æ£€ç´¢ä¼˜åŒ–
   - [ ] ç¼“å­˜æœºåˆ¶
   - [ ] å¼‚æ­¥å¤„ç†

## ğŸ“ˆ è¯„ä¼°æ•ˆæœçš„æ–¹æ³•

### 1. æ£€ç´¢è´¨é‡è¯„ä¼°

**æŒ‡æ ‡**:
- **Precisionï¼ˆç²¾ç¡®ç‡ï¼‰**: æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­ï¼Œç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹
- **Recallï¼ˆå¬å›ç‡ï¼‰**: æ‰€æœ‰ç›¸å…³æ–‡æ¡£ä¸­ï¼Œè¢«æ£€ç´¢åˆ°çš„æ¯”ä¾‹
- **F1 Score**: Precisionå’ŒRecallçš„è°ƒå’Œå¹³å‡

**ä½¿ç”¨æ–¹æ³•**:
```python
from backend.app.services.benchmark_evaluator import RAGBenchmarkEvaluator

# è¿è¡Œè¯„ä¼°
results = evaluator.evaluate()
print(results["summary"]["retrieval_metrics"])
```

### 2. ç”Ÿæˆè´¨é‡è¯„ä¼°

**æŒ‡æ ‡**:
- **Answer Similarityï¼ˆç­”æ¡ˆç›¸ä¼¼åº¦ï¼‰**: åŸºäºè¯æ±‡é‡å çš„ç›¸ä¼¼åº¦
- **Answer Length**: ç­”æ¡ˆé•¿åº¦ç»Ÿè®¡
- **Context Usage**: ä¸Šä¸‹æ–‡ä½¿ç”¨æƒ…å†µ

**æ”¹è¿›æ–¹å‘**:
- å¦‚æœPrecisionä½ â†’ é™ä½kå€¼æˆ–æé«˜ç›¸ä¼¼åº¦é˜ˆå€¼
- å¦‚æœRecallä½ â†’ å¢åŠ kå€¼æˆ–ä¼˜åŒ–embeddingæ¨¡å‹
- å¦‚æœç­”æ¡ˆè´¨é‡å·® â†’ ä¼˜åŒ–promptæ¨¡æ¿æˆ–å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦

### 3. ç«¯åˆ°ç«¯è¯„ä¼°

**è¯„ä¼°æµç¨‹**:
1. å‡†å¤‡benchmarkæ•°æ®é›†ï¼ˆåŒ…å«é—®é¢˜å’Œæ ‡å‡†ç­”æ¡ˆï¼‰
2. è¿è¡Œè¯„ä¼°è„šæœ¬
3. åˆ†ææŒ‡æ ‡ï¼Œè¯†åˆ«è–„å¼±ç¯èŠ‚
4. è°ƒæ•´å‚æ•°åé‡æ–°è¯„ä¼°
5. å¯¹æ¯”ä¸åŒé…ç½®çš„æ€§èƒ½

**ç¤ºä¾‹**:
```bash
# è¿è¡Œè¯„ä¼°
python -m backend.app.scripts.run_benchmark \
  --dataset benchmark_data/my_dataset.json \
  --collection documents \
  --k 4 \
  --output results.json

# æŸ¥çœ‹ç»“æœ
cat results.json | jq '.summary'
```

## ğŸ¯ è°ƒä¼˜å»ºè®®

### æ£€ç´¢ä¼˜åŒ–

1. **è°ƒæ•´kå€¼**
   - å°kï¼ˆ2-4ï¼‰: é«˜ç²¾ç¡®ç‡ï¼Œä½†å¯èƒ½é—æ¼ç›¸å…³ä¿¡æ¯
   - å¤§kï¼ˆ8-10ï¼‰: é«˜å¬å›ç‡ï¼Œä½†å¯èƒ½å¼•å…¥å™ªå£°

2. **ç›¸ä¼¼åº¦é˜ˆå€¼**
   - è®¾ç½®é˜ˆå€¼è¿‡æ»¤ä½è´¨é‡ç»“æœ
   - å¹³è¡¡æ£€ç´¢æ•°é‡å’Œç›¸å…³æ€§

3. **Chunkå¤§å°**
   - å°chunk: æ›´ç²¾ç¡®çš„æ£€ç´¢ï¼Œä½†å¯èƒ½ä¸¢å¤±ä¸Šä¸‹æ–‡
   - å¤§chunk: æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†å¯èƒ½åŒ…å«æ— å…³ä¿¡æ¯

4. **Embeddingæ¨¡å‹**
   - è€ƒè™‘ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹ï¼ˆå¦‚multilingualæ¨¡å‹ï¼‰
   - é’ˆå¯¹é¢†åŸŸè¿›è¡Œfine-tuning

### ç”Ÿæˆä¼˜åŒ–

1. **Promptæ¨¡æ¿**
   - æ˜ç¡®æŒ‡ç¤ºä½¿ç”¨ä¸Šä¸‹æ–‡
   - è¦æ±‚å¼•ç”¨æ¥æº
   - å¤„ç†"ä¸çŸ¥é“"çš„æƒ…å†µ

2. **ä¸Šä¸‹æ–‡ç®¡ç†**
   - å¹³è¡¡ä¸Šä¸‹æ–‡é•¿åº¦å’Œtokené™åˆ¶
   - ä¼˜å…ˆé€‰æ‹©é«˜ç›¸å…³æ€§çš„æ–‡æ¡£

3. **LLMå‚æ•°**
   - Temperature: 0.0-0.3 ç”¨äºäº‹å®æ€§å›ç­”
   - Max tokens: æ ¹æ®é¢„æœŸç­”æ¡ˆé•¿åº¦è®¾ç½®

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### åˆ›å»ºBenchmarkæ•°æ®é›†

```python
from backend.app.models.benchmark import BenchmarkDataset, BenchmarkQuestion

dataset = BenchmarkDataset(
    dataset_name="my_benchmark",
    description="Custom benchmark",
    questions=[
        BenchmarkQuestion(
            question_id="q001",
            question="What is the default chunk size?",
            ground_truth_answer="1000 characters",
            context_doc_ids=["doc_id_1"],
        )
    ],
)
```

### è¿è¡ŒRAGæŸ¥è¯¢

```python
from backend.app.services.rag_service import RAGService
from backend.app.services.vector_service import VectorService
from backend.app.services.embedding_service import EmbeddingService
from backend.app.services.llm_service import LLMService

# åˆå§‹åŒ–æœåŠ¡
rag_service = RAGService(
    vector_service=VectorService(),
    embedding_service=EmbeddingService(),
    llm_service=LLMService(),
    collection_name="documents",
    k=4,
)

# æŸ¥è¯¢
result = rag_service.query("What is RAG?")
print(result["answer"])
print(result["sources"])
```

### è¿è¡Œè¯„ä¼°

```python
from backend.app.services.benchmark_evaluator import (
    RAGBenchmarkEvaluator,
    load_benchmark_dataset,
)

# åŠ è½½æ•°æ®é›†
dataset = load_benchmark_dataset("benchmark_data/my_dataset.json")

# åˆ›å»ºè¯„ä¼°å™¨
evaluator = RAGBenchmarkEvaluator(rag_service=rag_service, dataset=dataset)

# è¿è¡Œè¯„ä¼°
results = evaluator.evaluate()
print(results["summary"])
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [RAG Benchmarkè¯¦ç»†æŒ‡å—](docs/RAG_BENCHMARK.md)
- [æ¶æ„è®¾è®¡](docs/ARCHITECTURE.md)

## ğŸš€ ä¸‹ä¸€æ­¥

1. **å‡†å¤‡æµ‹è¯•æ•°æ®**: åˆ›å»ºåŒ…å«çœŸå®æ–‡æ¡£å’Œé—®é¢˜çš„benchmarkæ•°æ®é›†
2. **è¿è¡ŒåŸºå‡†æµ‹è¯•**: åœ¨ç°æœ‰æ•°æ®é›†ä¸Šè¯„ä¼°å½“å‰é…ç½®
3. **è¯†åˆ«ç“¶é¢ˆ**: åˆ†ææŒ‡æ ‡ï¼Œæ‰¾å‡ºéœ€è¦æ”¹è¿›çš„åœ°æ–¹
4. **è¿­ä»£ä¼˜åŒ–**: è°ƒæ•´å‚æ•°ï¼Œé‡æ–°è¯„ä¼°ï¼Œå¯¹æ¯”æ•ˆæœ

