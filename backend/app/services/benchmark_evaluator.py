"""RAG benchmark evaluation system."""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional

from backend.app.models.benchmark import BenchmarkDataset, BenchmarkQuestion
from backend.app.services.embedding_service import EmbeddingService
from backend.app.services.llm_service import LLMService
from backend.app.services.rag_service import RAGService
from backend.app.services.vector_service import VectorService

logger = logging.getLogger(__name__)


class RAGEvaluationMetrics:
    """Metrics for RAG evaluation."""

    def __init__(self, retrieval_only: bool = False):
        """
        Initialize metrics.
        
        Args:
            retrieval_only: If True, skip answer-related metrics
        """
        self.total_questions = 0
        self.correct_answers = 0
        self.retrieval_precision = []
        self.retrieval_recall = []
        self.retrieval_f1 = []
        self.answer_similarity = []
        self.answer_length = []
        self.context_length = []
        self.retrieval_count = []
        self.retrieval_only = retrieval_only
        
        # Retrieval-specific metrics
        self.mean_reciprocal_rank = []
        self.mean_similarity_score = []
        self.top_k_accuracy = {1: [], 3: [], 5: []}

    def add_result(
        self,
        question: BenchmarkQuestion,
        predicted_answer: str,
        retrieved_doc_ids: List[str],
        context_length: int,
        sources: Optional[List[Dict]] = None,
    ):
        """
        Add evaluation result for a question.

        Args:
            question: Benchmark question
            predicted_answer: Answer generated by RAG system (empty if retrieval_only)
            retrieved_doc_ids: List of retrieved document IDs
            context_length: Length of context used
            sources: List of source dictionaries with distance/similarity scores
        """
        self.total_questions += 1
        
        if not self.retrieval_only:
            self.answer_length.append(len(predicted_answer))
            # Calculate answer similarity (simple word overlap)
            similarity = self._calculate_answer_similarity(
                question.ground_truth_answer, predicted_answer
            )
            self.answer_similarity.append(similarity)
        
        self.context_length.append(context_length)
        self.retrieval_count.append(len(retrieved_doc_ids))

        # Calculate retrieval metrics
        relevant_docs = set(question.context_doc_ids)
        retrieved_docs = set(retrieved_doc_ids)

        if len(retrieved_docs) > 0:
            precision = len(relevant_docs & retrieved_docs) / len(retrieved_docs)
            self.retrieval_precision.append(precision)
        else:
            self.retrieval_precision.append(0.0)

        if len(relevant_docs) > 0:
            recall = len(relevant_docs & retrieved_docs) / len(relevant_docs)
            self.retrieval_recall.append(recall)
        else:
            self.retrieval_recall.append(0.0)

        # Calculate F1
        if len(self.retrieval_precision) > 0 and len(self.retrieval_recall) > 0:
            p = self.retrieval_precision[-1]
            r = self.retrieval_recall[-1]
            f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0.0
            self.retrieval_f1.append(f1)

        # Calculate retrieval-specific metrics
        if sources:
            # Mean Reciprocal Rank (MRR)
            mrr = self._calculate_mrr(retrieved_doc_ids, relevant_docs)
            self.mean_reciprocal_rank.append(mrr)
            
            # Mean similarity score (convert distance to similarity)
            if sources:
                distances = [s.get("distance") for s in sources if s.get("distance") is not None]
                if distances:
                    # Convert distance to similarity (1 - distance for cosine distance)
                    similarities = [1.0 - d for d in distances]
                    self.mean_similarity_score.append(sum(similarities) / len(similarities))
            
            # Top-K accuracy
            for k in [1, 3, 5]:
                if len(retrieved_doc_ids) >= k:
                    top_k_docs = set(retrieved_doc_ids[:k])
                    accuracy = 1.0 if len(top_k_docs & relevant_docs) > 0 else 0.0
                    self.top_k_accuracy[k].append(accuracy)

    def _calculate_mrr(
        self, retrieved_doc_ids: List[str], relevant_docs: set
    ) -> float:
        """
        Calculate Mean Reciprocal Rank (MRR).
        
        Args:
            retrieved_doc_ids: List of retrieved document IDs in order
            relevant_docs: Set of relevant document IDs
            
        Returns:
            Reciprocal rank (1/rank of first relevant doc, or 0 if none found)
        """
        for rank, doc_id in enumerate(retrieved_doc_ids, start=1):
            if doc_id in relevant_docs:
                return 1.0 / rank
        return 0.0

    def _calculate_answer_similarity(self, answer1: str, answer2: str) -> float:
        """
        Calculate similarity between two answers.

        Args:
            answer1: First answer
            answer2: Second answer

        Returns:
            Similarity score between 0 and 1
        """
        # Simple word overlap similarity
        words1 = set(answer1.lower().split())
        words2 = set(answer2.lower().split())

        if not words1 or not words2:
            return 0.0

        intersection = words1 & words2
        union = words1 | words2

        return len(intersection) / len(union) if union else 0.0

    def get_summary(self) -> Dict:
        """
        Get summary statistics.

        Returns:
            Dictionary with summary metrics
        """
        if self.total_questions == 0:
            return {"error": "No questions evaluated"}

        summary = {
            "total_questions": self.total_questions,
            "retrieval_metrics": {
                "mean_precision": (
                    sum(self.retrieval_precision) / len(self.retrieval_precision)
                    if self.retrieval_precision
                    else 0.0
                ),
                "mean_recall": (
                    sum(self.retrieval_recall) / len(self.retrieval_recall)
                    if self.retrieval_recall
                    else 0.0
                ),
                "mean_f1": (
                    sum(self.retrieval_f1) / len(self.retrieval_f1)
                    if self.retrieval_f1
                    else 0.0
                ),
            },
            "context_metrics": {
                "mean_context_length": (
                    sum(self.context_length) / len(self.context_length)
                    if self.context_length
                    else 0.0
                ),
                "mean_retrieval_count": (
                    sum(self.retrieval_count) / len(self.retrieval_count)
                    if self.retrieval_count
                    else 0.0
                ),
            },
        }
        
        # Add retrieval-specific metrics
        if self.mean_reciprocal_rank:
            summary["retrieval_metrics"]["mean_reciprocal_rank"] = (
                sum(self.mean_reciprocal_rank) / len(self.mean_reciprocal_rank)
            )
        
        if self.mean_similarity_score:
            summary["retrieval_metrics"]["mean_similarity_score"] = (
                sum(self.mean_similarity_score) / len(self.mean_similarity_score)
            )
        
        # Top-K accuracy
        top_k_acc = {}
        for k in [1, 3, 5]:
            if self.top_k_accuracy[k]:
                top_k_acc[f"top_{k}_accuracy"] = (
                    sum(self.top_k_accuracy[k]) / len(self.top_k_accuracy[k])
                )
        if top_k_acc:
            summary["retrieval_metrics"]["top_k_accuracy"] = top_k_acc
        
        # Add answer metrics only if not retrieval_only
        if not self.retrieval_only:
            summary["answer_metrics"] = {
                "mean_similarity": (
                    sum(self.answer_similarity) / len(self.answer_similarity)
                    if self.answer_similarity
                    else 0.0
                ),
                "mean_answer_length": (
                    sum(self.answer_length) / len(self.answer_length)
                    if self.answer_length
                    else 0.0
                ),
            }
        
        return summary


class RAGBenchmarkEvaluator:
    """
    Evaluator for RAG benchmark datasets.
    
    Evaluates RAG system performance on benchmark questions and
    calculates retrieval and generation metrics.
    """

    def __init__(
        self,
        rag_service: RAGService,
        dataset: BenchmarkDataset,
        include_chunk_content: bool = True,
        retrieval_only: bool = False,
    ):
        """
        Initialize benchmark evaluator.

        Args:
            rag_service: RAGService instance to evaluate
            dataset: BenchmarkDataset to evaluate on
            include_chunk_content: Whether to include chunk text content in results
            retrieval_only: If True, skip LLM generation and only evaluate retrieval
        """
        self.rag_service = rag_service
        self.dataset = dataset
        self.retrieval_only = retrieval_only
        self.metrics = RAGEvaluationMetrics(retrieval_only=retrieval_only)
        self.include_chunk_content = include_chunk_content
        # Get vector_service from retriever for fetching chunk content
        self.vector_service = rag_service.retriever.vector_service
        # Resolve collection name (handle mapping if needed)
        collection_name = rag_service.retriever.collection_name
        if collection_name in self.vector_service.collection_names:
            self.collection_name = self.vector_service.collection_names[collection_name]
        else:
            self.collection_name = collection_name

        logger.info(
            f"Initialized RAG benchmark evaluator: dataset={dataset.dataset_name}, "
            f"questions={len(dataset.questions)}, "
            f"include_chunk_content={include_chunk_content}, "
            f"retrieval_only={retrieval_only}"
        )

    def _get_chunk_content(self, chunk_id: str) -> Optional[str]:
        """
        Get chunk text content from vector store.

        Args:
            chunk_id: Chunk ID to retrieve

        Returns:
            Chunk text content or None if not found
        """
        try:
            collection = self.vector_service.get_or_create_collection(
                self.collection_name
            )
            results = collection.get(ids=[chunk_id], include=["documents"])
            if results and results.get("documents") and len(results["documents"]) > 0:
                return results["documents"][0]
            return None
        except Exception as e:
            logger.warning(f"Error retrieving chunk content for {chunk_id}: {e}")
            return None

    def _enrich_sources_with_content(self, sources: List[Dict]) -> List[Dict]:
        """
        Enrich source information with chunk content.

        Args:
            sources: List of source dictionaries with chunk_id

        Returns:
            List of enriched source dictionaries with chunk_content
        """
        enriched_sources = []
        for source in sources:
            enriched_source = source.copy()
            if self.include_chunk_content:
                chunk_id = source.get("chunk_id")
                if chunk_id:
                    chunk_content = self._get_chunk_content(chunk_id)
                    enriched_source["chunk_content"] = chunk_content
            enriched_sources.append(enriched_source)
        return enriched_sources

    def evaluate(self, max_questions: Optional[int] = None) -> Dict:
        """
        Evaluate RAG system on benchmark dataset.

        Args:
            max_questions: Maximum number of questions to evaluate (None for all)

        Returns:
            Dictionary with evaluation results
        """
        questions_to_evaluate = self.dataset.questions[:max_questions]

        logger.info(
            f"Evaluating {len(questions_to_evaluate)} questions from "
            f"dataset '{self.dataset.dataset_name}'"
        )

        results = []
        self.metrics = RAGEvaluationMetrics(retrieval_only=self.retrieval_only)

        for i, question in enumerate(questions_to_evaluate):
            logger.info(
                f"Evaluating question {i+1}/{len(questions_to_evaluate)}: "
                f"{question.question_id}"
            )

            try:
                if self.retrieval_only:
                    # Only perform retrieval, skip LLM generation
                    documents = self.rag_service.retriever.get_relevant_documents(
                        question.question
                    )
                    
                    # Extract source information
                    sources = []
                    for doc in documents:
                        source_info = {
                            "chunk_id": doc.metadata.get("chunk_id"),
                            "title": doc.metadata.get("title", "Unknown"),
                            "doc_id": doc.metadata.get("doc_id"),
                            "distance": doc.metadata.get("distance"),
                        }
                        sources.append(source_info)
                    
                    retrieved_doc_ids = [
                        src.get("doc_id") for src in sources
                    ]
                    retrieved_doc_ids = [did for did in retrieved_doc_ids if did]
                    
                    # Format context for length calculation
                    context = self.rag_service._format_context(documents)
                    
                    # Add result to metrics
                    self.metrics.add_result(
                        question=question,
                        predicted_answer="",  # Empty for retrieval_only
                        retrieved_doc_ids=retrieved_doc_ids,
                        context_length=len(context),
                        sources=sources,
                    )
                    
                    # Enrich sources with chunk content if enabled
                    if self.include_chunk_content:
                        sources = self._enrich_sources_with_content(sources)
                    
                    # Store detailed result
                    result = {
                        "question_id": question.question_id,
                        "question": question.question,
                        "ground_truth_answer": question.ground_truth_answer,
                        "retrieved_doc_ids": retrieved_doc_ids,
                        "expected_doc_ids": question.context_doc_ids,
                        "sources": sources,
                        "metadata": {
                            "retrieved_count": len(documents),
                            "context_length": len(context),
                        },
                    }
                else:
                    # Full RAG evaluation with LLM generation
                    response = self.rag_service.query(question.question)

                    # Extract retrieved document IDs
                    retrieved_doc_ids = [
                        src.get("doc_id") for src in response.get("sources", [])
                    ]
                    retrieved_doc_ids = [did for did in retrieved_doc_ids if did]

                    # Enrich sources with chunk content if enabled
                    sources = response.get("sources", [])
                    if self.include_chunk_content:
                        sources = self._enrich_sources_with_content(sources)

                    # Add result to metrics
                    self.metrics.add_result(
                        question=question,
                        predicted_answer=response.get("answer", ""),
                        retrieved_doc_ids=retrieved_doc_ids,
                        context_length=response.get("metadata", {}).get(
                            "context_length", 0
                        ),
                        sources=sources,
                    )

                    # Store detailed result
                    result = {
                        "question_id": question.question_id,
                        "question": question.question,
                        "ground_truth_answer": question.ground_truth_answer,
                        "predicted_answer": response.get("answer", ""),
                        "retrieved_doc_ids": retrieved_doc_ids,
                        "expected_doc_ids": question.context_doc_ids,
                        "sources": sources,
                        "metadata": response.get("metadata", {}),
                    }
                
                results.append(result)

            except Exception as e:
                logger.error(f"Error evaluating question {question.question_id}: {e}")
                results.append(
                    {
                        "question_id": question.question_id,
                        "error": str(e),
                    }
                )

        summary = self.metrics.get_summary()

        return {
            "dataset_name": self.dataset.dataset_name,
            "total_questions": len(questions_to_evaluate),
            "summary": summary,
            "detailed_results": results,
        }

    def save_results(self, output_path: Path):
        """
        Save evaluation results to file.

        Args:
            output_path: Path to save results JSON file
        """
        results = self.evaluate()
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"Saved evaluation results to {output_path}")


def load_benchmark_dataset(file_path: Path) -> BenchmarkDataset:
    """
    Load benchmark dataset from JSON file.

    Args:
        file_path: Path to JSON file

    Returns:
        BenchmarkDataset instance
    """
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    return BenchmarkDataset(**data)


def save_benchmark_dataset(dataset: BenchmarkDataset, file_path: Path):
    """
    Save benchmark dataset to JSON file.

    Args:
        dataset: BenchmarkDataset instance
        file_path: Path to save JSON file
    """
    file_path.parent.mkdir(parents=True, exist_ok=True)

    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(dataset.model_dump(), f, indent=2, ensure_ascii=False)

    logger.info(f"Saved benchmark dataset to {file_path}")

