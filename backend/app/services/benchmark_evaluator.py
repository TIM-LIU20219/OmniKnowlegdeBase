"""RAG benchmark evaluation system."""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional

from backend.app.models.benchmark import BenchmarkDataset, BenchmarkQuestion
from backend.app.services.embedding_service import EmbeddingService
from backend.app.services.llm_service import LLMService
from backend.app.services.rag_service import RAGService
from backend.app.services.vector_service import VectorService

logger = logging.getLogger(__name__)


class RAGEvaluationMetrics:
    """Metrics for RAG evaluation."""

    def __init__(self):
        """Initialize metrics."""
        self.total_questions = 0
        self.correct_answers = 0
        self.retrieval_precision = []
        self.retrieval_recall = []
        self.retrieval_f1 = []
        self.answer_similarity = []
        self.answer_length = []
        self.context_length = []
        self.retrieval_count = []

    def add_result(
        self,
        question: BenchmarkQuestion,
        predicted_answer: str,
        retrieved_doc_ids: List[str],
        context_length: int,
    ):
        """
        Add evaluation result for a question.

        Args:
            question: Benchmark question
            predicted_answer: Answer generated by RAG system
            retrieved_doc_ids: List of retrieved document IDs
            context_length: Length of context used
        """
        self.total_questions += 1
        self.answer_length.append(len(predicted_answer))
        self.context_length.append(context_length)
        self.retrieval_count.append(len(retrieved_doc_ids))

        # Calculate retrieval metrics
        relevant_docs = set(question.context_doc_ids)
        retrieved_docs = set(retrieved_doc_ids)

        if len(retrieved_docs) > 0:
            precision = len(relevant_docs & retrieved_docs) / len(retrieved_docs)
            self.retrieval_precision.append(precision)
        else:
            self.retrieval_precision.append(0.0)

        if len(relevant_docs) > 0:
            recall = len(relevant_docs & retrieved_docs) / len(relevant_docs)
            self.retrieval_recall.append(recall)
        else:
            self.retrieval_recall.append(0.0)

        # Calculate F1
        if len(self.retrieval_precision) > 0 and len(self.retrieval_recall) > 0:
            p = self.retrieval_precision[-1]
            r = self.retrieval_recall[-1]
            f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0.0
            self.retrieval_f1.append(f1)

        # Calculate answer similarity (simple word overlap)
        similarity = self._calculate_answer_similarity(
            question.ground_truth_answer, predicted_answer
        )
        self.answer_similarity.append(similarity)

    def _calculate_answer_similarity(self, answer1: str, answer2: str) -> float:
        """
        Calculate similarity between two answers.

        Args:
            answer1: First answer
            answer2: Second answer

        Returns:
            Similarity score between 0 and 1
        """
        # Simple word overlap similarity
        words1 = set(answer1.lower().split())
        words2 = set(answer2.lower().split())

        if not words1 or not words2:
            return 0.0

        intersection = words1 & words2
        union = words1 | words2

        return len(intersection) / len(union) if union else 0.0

    def get_summary(self) -> Dict:
        """
        Get summary statistics.

        Returns:
            Dictionary with summary metrics
        """
        if self.total_questions == 0:
            return {"error": "No questions evaluated"}

        return {
            "total_questions": self.total_questions,
            "retrieval_metrics": {
                "mean_precision": (
                    sum(self.retrieval_precision) / len(self.retrieval_precision)
                    if self.retrieval_precision
                    else 0.0
                ),
                "mean_recall": (
                    sum(self.retrieval_recall) / len(self.retrieval_recall)
                    if self.retrieval_recall
                    else 0.0
                ),
                "mean_f1": (
                    sum(self.retrieval_f1) / len(self.retrieval_f1)
                    if self.retrieval_f1
                    else 0.0
                ),
            },
            "answer_metrics": {
                "mean_similarity": (
                    sum(self.answer_similarity) / len(self.answer_similarity)
                    if self.answer_similarity
                    else 0.0
                ),
                "mean_answer_length": (
                    sum(self.answer_length) / len(self.answer_length)
                    if self.answer_length
                    else 0.0
                ),
            },
            "context_metrics": {
                "mean_context_length": (
                    sum(self.context_length) / len(self.context_length)
                    if self.context_length
                    else 0.0
                ),
                "mean_retrieval_count": (
                    sum(self.retrieval_count) / len(self.retrieval_count)
                    if self.retrieval_count
                    else 0.0
                ),
            },
        }


class RAGBenchmarkEvaluator:
    """
    Evaluator for RAG benchmark datasets.
    
    Evaluates RAG system performance on benchmark questions and
    calculates retrieval and generation metrics.
    """

    def __init__(
        self,
        rag_service: RAGService,
        dataset: BenchmarkDataset,
    ):
        """
        Initialize benchmark evaluator.

        Args:
            rag_service: RAGService instance to evaluate
            dataset: BenchmarkDataset to evaluate on
        """
        self.rag_service = rag_service
        self.dataset = dataset
        self.metrics = RAGEvaluationMetrics()

        logger.info(
            f"Initialized RAG benchmark evaluator: dataset={dataset.dataset_name}, "
            f"questions={len(dataset.questions)}"
        )

    def evaluate(self, max_questions: Optional[int] = None) -> Dict:
        """
        Evaluate RAG system on benchmark dataset.

        Args:
            max_questions: Maximum number of questions to evaluate (None for all)

        Returns:
            Dictionary with evaluation results
        """
        questions_to_evaluate = self.dataset.questions[:max_questions]

        logger.info(
            f"Evaluating {len(questions_to_evaluate)} questions from "
            f"dataset '{self.dataset.dataset_name}'"
        )

        results = []
        self.metrics = RAGEvaluationMetrics()

        for i, question in enumerate(questions_to_evaluate):
            logger.info(
                f"Evaluating question {i+1}/{len(questions_to_evaluate)}: "
                f"{question.question_id}"
            )

            try:
                # Query RAG system
                response = self.rag_service.query(question.question)

                # Extract retrieved document IDs
                retrieved_doc_ids = [
                    src.get("doc_id") for src in response.get("sources", [])
                ]
                retrieved_doc_ids = [did for did in retrieved_doc_ids if did]

                # Add result to metrics
                self.metrics.add_result(
                    question=question,
                    predicted_answer=response.get("answer", ""),
                    retrieved_doc_ids=retrieved_doc_ids,
                    context_length=response.get("metadata", {}).get(
                        "context_length", 0
                    ),
                )

                # Store detailed result
                result = {
                    "question_id": question.question_id,
                    "question": question.question,
                    "ground_truth_answer": question.ground_truth_answer,
                    "predicted_answer": response.get("answer", ""),
                    "retrieved_doc_ids": retrieved_doc_ids,
                    "expected_doc_ids": question.context_doc_ids,
                    "sources": response.get("sources", []),
                    "metadata": response.get("metadata", {}),
                }
                results.append(result)

            except Exception as e:
                logger.error(f"Error evaluating question {question.question_id}: {e}")
                results.append(
                    {
                        "question_id": question.question_id,
                        "error": str(e),
                    }
                )

        summary = self.metrics.get_summary()

        return {
            "dataset_name": self.dataset.dataset_name,
            "total_questions": len(questions_to_evaluate),
            "summary": summary,
            "detailed_results": results,
        }

    def save_results(self, output_path: Path):
        """
        Save evaluation results to file.

        Args:
            output_path: Path to save results JSON file
        """
        results = self.evaluate()
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"Saved evaluation results to {output_path}")


def load_benchmark_dataset(file_path: Path) -> BenchmarkDataset:
    """
    Load benchmark dataset from JSON file.

    Args:
        file_path: Path to JSON file

    Returns:
        BenchmarkDataset instance
    """
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    return BenchmarkDataset(**data)


def save_benchmark_dataset(dataset: BenchmarkDataset, file_path: Path):
    """
    Save benchmark dataset to JSON file.

    Args:
        dataset: BenchmarkDataset instance
        file_path: Path to save JSON file
    """
    file_path.parent.mkdir(parents=True, exist_ok=True)

    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(dataset.model_dump(), f, indent=2, ensure_ascii=False)

    logger.info(f"Saved benchmark dataset to {file_path}")

