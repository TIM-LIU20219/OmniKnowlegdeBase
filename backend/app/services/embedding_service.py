"""Embedding service for generating document embeddings."""

import logging
from typing import List, Optional

try:
    import torch
except ImportError:
    torch = None

from sentence_transformers import SentenceTransformer

from backend.app.utils.embedding_config import (
    EmbeddingProvider,
    embedding_config,
)

logger = logging.getLogger(__name__)


class EmbeddingService:
    """Service for generating text embeddings using unified configuration."""

    def __init__(
        self,
        model_name: Optional[str] = None,
        device: Optional[str] = None,
        preload: bool = False,
        config=None,
    ):
        """
        Initialize embedding service.

        Args:
            model_name: Name of the sentence transformer model to use.
                       If None, uses global embedding_config.
            device: Device to use ('cpu', 'cuda', or None for auto-detect).
                   If None, uses global embedding_config device setting.
            preload: If True, load the model immediately. Default False (lazy loading).
            config: Optional EmbeddingConfig instance. If None, uses global embedding_config.
        """
        # Use provided config or global config
        self.config = config if config is not None else embedding_config

        # Only support LOCAL provider for now (sentence-transformers)
        if self.config.provider != EmbeddingProvider.LOCAL:
            raise ValueError(
                f"EmbeddingService only supports LOCAL provider. "
                f"Current provider: {self.config.provider.value}. "
                "For cloud providers, use their respective API clients."
            )

        # Use provided model_name or config model_name
        if model_name is None:
            model_name = self.config.model_name
        self.model_name = model_name

        # Device selection: parameter > config > auto-detect
        if device is None:
            device = self.config.device

        if device == "auto":
            # Auto-detect: use GPU if available, otherwise CPU
            if torch is not None and torch.cuda.is_available():
                device = "cuda"
            else:
                device = "cpu"

        self.device = device
        self._model: Optional[SentenceTransformer] = None

        logger.info(
            f"Initialized embedding service: model={model_name}, "
            f"device={device}, dimension={self.config.dimension}"
        )
        if device == "cuda" and torch is not None:
            logger.info(f"Using GPU: {torch.cuda.get_device_name(0)}")

        # Preload model if requested
        if preload:
            _ = self.model  # Trigger model loading

    @property
    def model(self) -> SentenceTransformer:
        """
        Get or load the embedding model.

        Returns:
            SentenceTransformer model instance
        """
        if self._model is None:
            try:
                logger.info(f"Loading embedding model: {self.model_name}")
                self._model = SentenceTransformer(self.model_name, device=self.device)
                logger.info(f"Embedding model loaded successfully on {self.device}")
            except Exception as e:
                logger.error(f"Error loading embedding model: {e}")
                raise

        return self._model

    def embed_text(self, text: str) -> List[float]:
        """
        Generate embedding for a single text.

        Args:
            text: Text to embed

        Returns:
            List of embedding values
        """
        try:
            embedding = self.model.encode(text, convert_to_numpy=True)
            return embedding.tolist()
        except Exception as e:
            logger.error(f"Error generating embedding: {e}")
            raise

    def embed_texts(
        self, texts: List[str], batch_size: Optional[int] = None
    ) -> List[List[float]]:
        """
        Generate embeddings for multiple texts.

        Args:
            texts: List of texts to embed
            batch_size: Batch size for processing. If None, uses device-optimized default:
                       - GPU: 128 (better GPU utilization)
                       - CPU: 32 (memory efficient)

        Returns:
            List of embeddings (each is a list of floats)
        """
        if not texts:
            return []

        # Optimize batch size based on device
        if batch_size is None:
            batch_size = 128 if self.device == "cuda" else 32

        try:
            embeddings = self.model.encode(
                texts, batch_size=batch_size, convert_to_numpy=True
            )
            return embeddings.tolist()
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            raise

    def get_embedding_dimension(self) -> int:
        """
        Get the dimension of embeddings generated by this model.

        Returns:
            Embedding dimension from unified config
        """
        # Use dimension from config to avoid unnecessary model loading
        return self.config.dimension

