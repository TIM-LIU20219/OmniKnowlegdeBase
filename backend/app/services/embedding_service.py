"""Embedding service for generating document embeddings."""

import logging
import os
from typing import List, Optional

try:
    import torch
except ImportError:
    torch = None

from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)

# Default embedding model (中文模型)
DEFAULT_MODEL = "BAAI/bge-small-zh-v1.5"


class EmbeddingService:
    """Service for generating text embeddings."""

    def __init__(
        self, model_name: Optional[str] = None, device: Optional[str] = None, preload: bool = False
    ):
        """
        Initialize embedding service.

        Args:
            model_name: Name of the sentence transformer model to use.
                       If None, uses DEFAULT_MODEL or EMBEDDING_MODEL env var.
            device: Device to use ('cpu', 'cuda', or None for auto-detect).
                   If None, uses EMBEDDING_DEVICE env var or auto-detects.
            preload: If True, load the model immediately. Default False (lazy loading).
        """
        if model_name is None:
            model_name = os.getenv("EMBEDDING_MODEL", DEFAULT_MODEL)

        # Device selection: env var > parameter > auto-detect
        if device is None:
            device = os.getenv("EMBEDDING_DEVICE", "auto")

        if device == "auto":
            # Auto-detect: use GPU if available, otherwise CPU
            if torch is not None and torch.cuda.is_available():
                device = "cuda"
            else:
                device = "cpu"

        self.model_name = model_name
        self.device = device
        self._model: Optional[SentenceTransformer] = None

        logger.info(
            f"Initialized embedding service: model={model_name}, device={device}"
        )
        if device == "cuda" and torch is not None:
            logger.info(f"Using GPU: {torch.cuda.get_device_name(0)}")
        
        # Preload model if requested
        if preload:
            _ = self.model  # Trigger model loading

    @property
    def model(self) -> SentenceTransformer:
        """
        Get or load the embedding model.

        Returns:
            SentenceTransformer model instance
        """
        if self._model is None:
            try:
                logger.info(f"Loading embedding model: {self.model_name}")
                self._model = SentenceTransformer(self.model_name, device=self.device)
                logger.info(f"Embedding model loaded successfully on {self.device}")
            except Exception as e:
                logger.error(f"Error loading embedding model: {e}")
                raise

        return self._model

    def embed_text(self, text: str) -> List[float]:
        """
        Generate embedding for a single text.

        Args:
            text: Text to embed

        Returns:
            List of embedding values
        """
        try:
            embedding = self.model.encode(text, convert_to_numpy=True)
            return embedding.tolist()
        except Exception as e:
            logger.error(f"Error generating embedding: {e}")
            raise

    def embed_texts(
        self, texts: List[str], batch_size: Optional[int] = None
    ) -> List[List[float]]:
        """
        Generate embeddings for multiple texts.

        Args:
            texts: List of texts to embed
            batch_size: Batch size for processing. If None, uses device-optimized default:
                       - GPU: 128 (better GPU utilization)
                       - CPU: 32 (memory efficient)

        Returns:
            List of embeddings (each is a list of floats)
        """
        if not texts:
            return []

        # Optimize batch size based on device
        if batch_size is None:
            batch_size = 128 if self.device == "cuda" else 32

        try:
            embeddings = self.model.encode(
                texts, batch_size=batch_size, convert_to_numpy=True
            )
            return embeddings.tolist()
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            raise

    def get_embedding_dimension(self) -> int:
        """
        Get the dimension of embeddings generated by this model.

        Returns:
            Embedding dimension
        """
        # Test with a dummy text to get dimension
        test_embedding = self.embed_text("test")
        return len(test_embedding)

