"""
éªŒæ”¶è„šæœ¬ï¼šæµ‹è¯•PDFæ–‡æœ¬æ¸…æ´—å’Œåˆ†å—æ•ˆæœ

è¯¥è„šæœ¬ä¼šï¼š
1. æå–PDFåŸå§‹æ–‡æœ¬
2. å±•ç¤ºæ¸…æ´—å‰åçš„å¯¹æ¯”
3. å±•ç¤ºåˆ†å—ç»“æœ
4. å°†ç»“æœä¿å­˜ä¸ºmarkdownæ–‡ä»¶

Usage:
    python backend/app/utils/validate_cleaning_chunking.py <pdf_path>
    # OR from project root:
    python -m backend.app.utils.validate_cleaning_chunking <pdf_path>
"""

import argparse
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, Tuple

# Add project root to Python path
script_dir = Path(__file__).resolve().parent
project_root = script_dir.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from backend.app.services.chunking_service import ChunkingService, ChunkingStrategy
from backend.app.utils.pdf_extractor import PDFExtractor
from backend.app.utils.text_cleaner import TextCleaner


def extract_raw_pdf_text(pdf_path: Path, backend: Optional[str] = None) -> Tuple[str, str]:
    """
    æå–PDFåŸå§‹æ–‡æœ¬ï¼ˆæœªæ¸…æ´—ï¼‰ï¼Œæ”¯æŒå¤šç§åç«¯ã€‚

    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        backend: å¯é€‰çš„åç«¯é€‰æ‹© ('pymupdf', 'pdfplumber', 'pypdf')

    Returns:
        (åŸå§‹æå–çš„æ–‡æœ¬, ä½¿ç”¨çš„åç«¯)
    """
    extractor = PDFExtractor(preferred_backend=backend)
    text, backend_used = extractor.extract_with_fallback(pdf_path)
    return text, backend_used


def generate_report(
    pdf_path: Path,
    raw_text: str,
    cleaned_text: str,
    chunks: list,
    output_path: Path,
    backend_used: str = "unknown",
    strategy: str = "unknown",
) -> None:
    """
    ç”Ÿæˆmarkdownæ ¼å¼çš„éªŒæ”¶æŠ¥å‘Šã€‚

    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        raw_text: åŸå§‹æ–‡æœ¬
        cleaned_text: æ¸…æ´—åçš„æ–‡æœ¬
        chunks: åˆ†å—åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    raw_lines = len(raw_text.split("\n"))
    cleaned_lines = len(cleaned_text.split("\n"))
    raw_chars = len(raw_text)
    cleaned_chars = len(cleaned_text)
    reduction_ratio = (
        (raw_chars - cleaned_chars) / raw_chars * 100 if raw_chars > 0 else 0
    )
    
    # æ£€æµ‹è¢«è¿‡æ»¤çš„ç‰¹æ®Šå­—æ®µï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œåªæ˜¾ç¤ºç¤ºä¾‹ï¼‰
    # æ³¨æ„ï¼šç”±äºæ¸…æ´—å¯èƒ½æ”¹å˜è¡Œçš„æ ¼å¼ï¼Œè¿™é‡Œåªåšç®€å•å¯¹æ¯”
    filtered_lines = []
    raw_lines_set = {line.strip() for line in raw_text.split("\n") if line.strip()}
    cleaned_lines_set = {line.strip() for line in cleaned_text.split("\n") if line.strip()}
    filtered_lines = list(raw_lines_set - cleaned_lines_set)[:10]  # åªå–å‰10ä¸ªç¤ºä¾‹
    
    # ç”Ÿæˆmarkdownå†…å®¹
    markdown_content = f"""# PDFæ–‡æœ¬æ¸…æ´—å’Œåˆ†å—éªŒæ”¶æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯

- **PDFæ–‡ä»¶**: `{pdf_path.name}`
- **æ–‡ä»¶è·¯å¾„**: `{pdf_path}`
- **ç”Ÿæˆæ—¶é—´**: {timestamp}
- **PDFæå–åç«¯**: `{backend_used}`
- **åˆ†å—ç­–ç•¥**: `{strategy}`

## ç»Ÿè®¡ä¿¡æ¯

### æ–‡æœ¬ç»Ÿè®¡

| æŒ‡æ ‡ | åŸå§‹æ–‡æœ¬ | æ¸…æ´—åæ–‡æœ¬ | å˜åŒ– |
|------|---------|-----------|------|
| å­—ç¬¦æ•° | {raw_chars:,} | {cleaned_chars:,} | -{raw_chars - cleaned_chars:,} ({reduction_ratio:.1f}%) |
| è¡Œæ•° | {raw_lines:,} | {cleaned_lines:,} | -{raw_lines - cleaned_lines:,} |
| åˆ†å—æ•°é‡ | - | {len(chunks)} | - |

### åˆ†å—ç»Ÿè®¡

- **æ€»chunkæ•°**: {len(chunks)}
- **å¹³å‡chunké•¿åº¦**: {sum(len(c) for c in chunks) / len(chunks):.0f} å­—ç¬¦
- **æœ€å¤§chunké•¿åº¦**: {max(len(c) for c in chunks) if chunks else 0} å­—ç¬¦
- **æœ€å°chunké•¿åº¦**: {min(len(c) for c in chunks) if chunks else 0} å­—ç¬¦

## 1. åŸå§‹æ–‡æœ¬ï¼ˆå‰500å­—ç¬¦ï¼‰

```
{raw_text[:500]}
...
```

## 2. æ¸…æ´—åæ–‡æœ¬ï¼ˆå‰500å­—ç¬¦ï¼‰

```
{cleaned_text[:500]}
...
```

## 3. æ–‡æœ¬å¯¹æ¯”

### è¢«è¿‡æ»¤çš„å†…å®¹ç¤ºä¾‹

ä»¥ä¸‹æ˜¯è¢«æ¸…æ´—è¿‡ç¨‹è¿‡æ»¤æ‰çš„å†…å®¹ï¼ˆå‰10è¡Œï¼‰ï¼š

"""
    
    # æ·»åŠ è¢«è¿‡æ»¤çš„è¡Œ
    if filtered_lines:
        filtered_sample = list(filtered_lines)[:10]
        for i, line in enumerate(filtered_sample, 1):
            markdown_content += f"{i}. `{line[:100]}{'...' if len(line) > 100 else ''}`\n"
    else:
        markdown_content += "æ— è¢«è¿‡æ»¤çš„å†…å®¹ã€‚\n"
    
    markdown_content += f"""
## 4. åˆ†å—ç»“æœ

å…±ç”Ÿæˆ **{len(chunks)}** ä¸ªchunkï¼š

"""
    
    # æ·»åŠ æ¯ä¸ªchunkçš„è¯¦ç»†ä¿¡æ¯
    for i, chunk in enumerate(chunks, 1):
        chunk_preview = chunk[:200] + "..." if len(chunk) > 200 else chunk
        is_reference = TextCleaner.is_reference_section(chunk)
        reference_marker = " âš ï¸ **å‚è€ƒæ–‡çŒ®section**" if is_reference else ""
        
        markdown_content += f"""### Chunk {i}{reference_marker}

- **é•¿åº¦**: {len(chunk)} å­—ç¬¦
- **é¢„è§ˆ**:
```
{chunk_preview}
```

"""
    
    # æ·»åŠ å®Œæ•´æ–‡æœ¬å¯¹æ¯”ï¼ˆå¦‚æœä¸å¤ªé•¿ï¼‰
    if raw_chars < 10000:
        markdown_content += f"""
## 5. å®Œæ•´æ–‡æœ¬å¯¹æ¯”

### åŸå§‹æ–‡æœ¬ï¼ˆå®Œæ•´ï¼‰

```
{raw_text}
```

### æ¸…æ´—åæ–‡æœ¬ï¼ˆå®Œæ•´ï¼‰

```
{cleaned_text}
```

"""
    
    # ä¿å­˜æ–‡ä»¶
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(markdown_content, encoding="utf-8")
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")


def validate_pdf(
    pdf_path: Path,
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    output_dir: Path = None,
    strategy: str = ChunkingStrategy.HYBRID,
) -> Tuple[str, str, list]:
    """
    éªŒè¯PDFçš„æ–‡æœ¬æ¸…æ´—å’Œåˆ†å—æ•ˆæœã€‚

    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        chunk_size: åˆ†å—å¤§å°
        chunk_overlap: åˆ†å—é‡å å¤§å°
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨PDFæ‰€åœ¨ç›®å½•ï¼‰

    Returns:
        (raw_text, cleaned_text, chunks) å…ƒç»„
    """
    print("=" * 80)
    print("PDFæ–‡æœ¬æ¸…æ´—å’Œåˆ†å—éªŒæ”¶")
    print("=" * 80)
    print(f"\nå¤„ç†æ–‡ä»¶: {pdf_path}")
    
    if not pdf_path.exists():
        raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
    
    # 1. æå–åŸå§‹æ–‡æœ¬
    print("\n[1/4] æå–PDFåŸå§‹æ–‡æœ¬...")
    raw_text, backend_used = extract_raw_pdf_text(pdf_path)
    print(f"   âœ… æå–å®Œæˆ: {len(raw_text):,} å­—ç¬¦, {len(raw_text.split('\n')):,} è¡Œ")
    print(f"   ğŸ“š ä½¿ç”¨çš„åç«¯: {backend_used}")
    
    # 2. æ¸…æ´—æ–‡æœ¬
    print("\n[2/4] æ¸…æ´—æ–‡æœ¬...")
    cleaned_text = TextCleaner.clean_pdf_text(raw_text)
    print(f"   âœ… æ¸…æ´—å®Œæˆ: {len(cleaned_text):,} å­—ç¬¦, {len(cleaned_text.split('\n')):,} è¡Œ")
    reduction = len(raw_text) - len(cleaned_text)
    if reduction > 0:
        print(f"   ğŸ“‰ å‡å°‘äº† {reduction:,} å­—ç¬¦ ({reduction/len(raw_text)*100:.1f}%)")
    
    # 3. åˆ†å—
    print("\n[3/4] åˆ†å—æ–‡æœ¬...")
    print(f"   ğŸ“‹ åˆ†å—ç­–ç•¥: {strategy}")
    chunking_service = ChunkingService(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap, strategy=strategy
    )
    chunks = chunking_service.chunk_text(cleaned_text)
    print(f"   âœ… åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªchunk")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å‚è€ƒæ–‡çŒ®sectionè¢«è¿‡æ»¤
    reference_chunks = [
        i for i, chunk in enumerate(chunks, 1) 
        if TextCleaner.is_reference_section(chunk)
    ]
    if reference_chunks:
        print(f"   âš ï¸  æ£€æµ‹åˆ° {len(reference_chunks)} ä¸ªå‚è€ƒæ–‡çŒ®section: {reference_chunks}")
    else:
        print(f"   âœ… æœªæ£€æµ‹åˆ°å‚è€ƒæ–‡çŒ®section")
    
    # 4. ç”ŸæˆæŠ¥å‘Š
    print("\n[4/4] ç”ŸæˆéªŒæ”¶æŠ¥å‘Š...")
    if output_dir is None:
        output_dir = pdf_path.parent
    
    output_filename = f"{pdf_path.stem}_cleaning_chunking_report.md"
    output_path = output_dir / output_filename
    
    generate_report(pdf_path, raw_text, cleaned_text, chunks, output_path, backend_used, strategy)
    
    print("\n" + "=" * 80)
    print("âœ… éªŒæ”¶å®Œæˆï¼")
    print("=" * 80)
    print(f"\nğŸ“„ æŠ¥å‘Šä½ç½®: {output_path}")
    print(f"\nğŸ“Š æ‘˜è¦:")
    print(f"   - åŸå§‹æ–‡æœ¬: {len(raw_text):,} å­—ç¬¦")
    print(f"   - æ¸…æ´—å: {len(cleaned_text):,} å­—ç¬¦")
    print(f"   - åˆ†å—æ•°é‡: {len(chunks)}")
    print(f"   - å¹³å‡chunké•¿åº¦: {sum(len(c) for c in chunks) / len(chunks):.0f} å­—ç¬¦")
    
    return raw_text, cleaned_text, chunks


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="éªŒæ”¶PDFæ–‡æœ¬æ¸…æ´—å’Œåˆ†å—æ•ˆæœ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "pdf_path",
        type=str,
        help="PDFæ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=1000,
        help="åˆ†å—å¤§å°ï¼ˆé»˜è®¤: 1000ï¼‰",
    )
    parser.add_argument(
        "--chunk-overlap",
        type=int,
        default=200,
        help="åˆ†å—é‡å å¤§å°ï¼ˆé»˜è®¤: 200ï¼‰",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: PDFæ‰€åœ¨ç›®å½•ï¼‰",
    )
    parser.add_argument(
        "--strategy",
        type=str,
        default=ChunkingStrategy.HYBRID,
        choices=["character", "sentence", "paragraph", "hybrid"],
        help=f"åˆ†å—ç­–ç•¥ï¼ˆé»˜è®¤: {ChunkingStrategy.HYBRID}ï¼‰",
    )
    
    args = parser.parse_args()
    
    pdf_path = Path(args.pdf_path).resolve()
    output_dir = Path(args.output_dir).resolve() if args.output_dir else None
    
    try:
        validate_pdf(
            pdf_path=pdf_path,
            chunk_size=args.chunk_size,
            chunk_overlap=args.chunk_overlap,
            output_dir=output_dir,
            strategy=args.strategy,
        )
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

